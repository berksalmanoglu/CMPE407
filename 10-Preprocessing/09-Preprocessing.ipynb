{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "* Preprocessing is the procedure of preparing data so it can be used for machine learning.\n",
    "\n",
    "* Preprocessing is important because in geneal learning algorithms benefit from standardization and normalization of a data set.\n",
    "\n",
    "* Scikit Learns sklearn.preprocessing package provides a lot of different preprocessing functions. We will take a look at standardization, normalization, encoding categorical features, imputation of missing values as well as generating polynomial features for algorithms like LinearRegression.\n",
    "\n",
    "An important thing for almost all preprocessing methods is that we don’t fit it on the whole data but rather only on the training set and transform both the training and testing set with the scaler learned with only the training set.\n",
    "\n",
    "To see how preprocessing works we first of all need to load in a dataset which we can perform preprocessing on. For that we will use the famous Iris dataset because it has different scales as well as an non-numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'])\n",
    "X = np.array(iris.drop(['label'], axis=1))\n",
    "y = np.array(iris['label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "iris.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "We will start of by taking a look at standardization.\n",
    "\n",
    "Standardization of datasets is a really common proceature. For some algorithm and datasets even required. Algorithms like SVM with RBF kernel or Ridge and Lasso Regression assume that all features are standardized. For algorithms like Nearest Neighbors it’s important that features are scaled because you compute the distance.\n",
    "\n",
    "An easy way to standardize our dataset is Scikit Learns _**StandardScaler**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.80916667 3.0575     3.7275     1.1825    ]\n",
      "[0.82036535 0.44453393 1.7439401  0.75029578]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.47393679,  1.22037928, -1.5639872 , -1.30948358],\n",
       "       [-0.13307079,  3.02001693, -1.27728011, -1.04292204],\n",
       "       [ 1.08589829,  0.09560575,  0.38562104,  0.28988568],\n",
       "       [-1.23014297,  0.77046987, -1.21993869, -1.30948358],\n",
       "       [-1.7177306 ,  0.32056046, -1.39196294, -1.30948358]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above we first imported the StandardScaler and then fit it on the training features. The we printed a few informations about the scaler. And lastly we transformed our training features and printed the first 5 rows to the console.\n",
    "\n",
    "Now we need to transform our testing features using the same scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35451684, -0.57925837,  0.5576453 ,  0.02332414],\n",
       "       [-0.13307079,  1.67028869, -1.16259727, -1.17620281],\n",
       "       [ 2.30486738, -1.02916778,  1.81915651,  1.48941263],\n",
       "       [ 0.23261993, -0.35430366,  0.44296246,  0.42316645],\n",
       "       [ 1.2077952 , -0.57925837,  0.61498672,  0.28988568]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the standardization and normalization algorithms we will use in this tutorial will have the same logic.\n",
    "\n",
    "1. Fitting on the training dataset\n",
    "2. Transforming the training dataset\n",
    "3. Transforming the testing dataset\n",
    "\n",
    "If we want our values between a minimum and maximum value we can use the _**MinMaxScaler**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08823529, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.41176471, 1.        , 0.0877193 , 0.125     ],\n",
       "       [0.70588235, 0.45833333, 0.59649123, 0.54166667],\n",
       "       [0.14705882, 0.58333333, 0.10526316, 0.04166667],\n",
       "       [0.02941176, 0.5       , 0.05263158, 0.04166667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52941176, 0.33333333, 0.64912281, 0.45833333],\n",
       "       [0.41176471, 0.75      , 0.12280702, 0.08333333],\n",
       "       [1.        , 0.25      , 1.03508772, 0.91666667],\n",
       "       [0.5       , 0.375     , 0.61403509, 0.58333333],\n",
       "       [0.73529412, 0.33333333, 0.66666667, 0.54166667]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the only syntactical difference is that we need to specify a feature_range.\n",
    "\n",
    "The last method of standardization we will talk about is calling each feature by its maximum absolute value using _**MaxAbsScaler**_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5974026 , 0.81818182, 0.14925373, 0.08      ],\n",
       "       [0.74025974, 1.        , 0.2238806 , 0.16      ],\n",
       "       [0.87012987, 0.70454545, 0.65671642, 0.56      ],\n",
       "       [0.62337662, 0.77272727, 0.23880597, 0.08      ],\n",
       "       [0.57142857, 0.72727273, 0.19402985, 0.08      ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79220779, 0.63636364, 0.70149254, 0.48      ],\n",
       "       [0.74025974, 0.86363636, 0.25373134, 0.12      ],\n",
       "       [1.        , 0.59090909, 1.02985075, 0.92      ],\n",
       "       [0.77922078, 0.65909091, 0.67164179, 0.6       ],\n",
       "       [0.88311688, 0.63636364, 0.71641791, 0.56      ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we scaled it by the absolute maximum value the largest value of the training set will be one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Normalization is the process of scaling individual samples to have unit norm.\n",
    "\n",
    "Scikit Learns _**Normalizer**_ allows us to normalize our features. We only need to pass it the norm we want to use to normalize the dataset. In the example we are using the L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77577075, 0.60712493, 0.16864581, 0.03372916],\n",
       "       [0.77381111, 0.59732787, 0.2036345 , 0.05430253],\n",
       "       [0.76945444, 0.35601624, 0.50531337, 0.16078153],\n",
       "       [0.786991  , 0.55745196, 0.26233033, 0.03279129],\n",
       "       [0.78609038, 0.57170209, 0.23225397, 0.03573138]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer(norm='l2') # norm specifies the norm used to normalize the data\n",
    "X_train_normalized = normalizer.fit_transform(X_train)\n",
    "X_train_normalized[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73659895, 0.33811099, 0.56754345, 0.14490471],\n",
       "       [0.8068282 , 0.53788547, 0.24063297, 0.04246464],\n",
       "       [0.70600618, 0.2383917 , 0.63265489, 0.21088496],\n",
       "       [0.73350949, 0.35452959, 0.55013212, 0.18337737],\n",
       "       [0.76467269, 0.31486523, 0.53976896, 0.15743261]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_normalized = normalizer.transform(X_test)\n",
    "X_test_normalized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "Sometimes our data isn’t in the format we want or need to use it for Machine Learning. An example is categorical data that is represented as a string like the label column of our Iris dataset. We can use something like Scikit Learns LabelEncoder or One Hot Encoding to encode our data into a useful format.\n",
    "\n",
    "First of we will print the first 5 labels so you can see that they are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of we will take a look at how Scikit Learns _**LabelEncoder**_ works. It can be used to transforms categorical textual data into numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_le = le.fit_transform(y)\n",
    "y_le[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next of we will take a look at one hot encoding which is really popular for classification problems using Neural Networks.\n",
    "\n",
    "_**One Hot Encoding**_ transforms categorical data into arrays of booleans where only one of them is true(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berk\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform([[1], [2], [3]]).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing values\n",
    "Many real world datasets contain missing values, often encodes as blanks, NaNs or other placeholders. To use datasets in Scikit Learn we need to fill in those missing values. A basic strategy is to just delete all rows with missing values but this comes at the cost of losing all the information contained in this row. A better way is to impute the missing values. You can impute missing values with something like the columns mean, median or most frequent value.\n",
    "\n",
    "Scikit Learn provides an _**Imputer**_. We only need to pass it what values are missing and a strategy to replace them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berk\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [7., 8.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean')\n",
    "imp.fit_transform([[1, 2], [7, 8], [np.nan, np.nan]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating polynomial features##\n",
    "Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple way of doing so is to use polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  2.,  3.,  1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5.,  6., 16., 20., 24., 25., 30., 36.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = [[1,2,3], [4,5,6]]\n",
    "print(X)\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://gilberttanner.com/2018/09/17/scikit-learn-tutorial-12-preprocessing/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
